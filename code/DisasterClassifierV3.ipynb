{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1491c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = './../dataset'\n",
    "\n",
    "classes = [\n",
    "    'Cyclone', \n",
    "    'Earthquake', \n",
    "    'Flood', \n",
    "    'Wildfire'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1318d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.80            #80% from total dataset\n",
    "validation_split = 0.125      #12.5% from training split\n",
    "test_split = 0.20             #20% from total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9009cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some hyper parameters\n",
    "batch_size = 32\n",
    "step_size = 8\n",
    "number_of_epochs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec90417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies are added\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# import matplotlib to draw figure\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# importing necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dense, Flatten, Add, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNetRS152\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print('All dependencies are added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f4355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all images path\n",
      "Done!!\n",
      "Loaded  4428 images path\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading all images path\")\n",
    "imagePaths = list(paths.list_images(data_path))\n",
    "data = []\n",
    "labels = []\n",
    "print(\"Done!!\")\n",
    "print(\"Loaded \", len(imagePaths), \"images path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f588d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing data...\n",
      "All images labeled\n"
     ]
    }
   ],
   "source": [
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, convert it to RGB channel ordering, and resize\n",
    "    # it to be a fixed 224x224 pixels, ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "print(\"[INFO] processing data...\")\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "print(\"All images labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318bdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=test_split, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7fd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the validation split from the training split\n",
    "(trainX, valX, trainY, valY) = train_test_split(trainX, trainY, test_size=validation_split, random_state=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab3fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet model\n",
    "resnet = ResNetRS152(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the weights of the ResNet model\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the ResNet model\n",
    "num_classes = len(classes)\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d70b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fdaa325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "97/97 [==============================] - 226s 2s/step - loss: 0.2232 - accuracy: 0.9248 - val_loss: 0.1217 - val_accuracy: 0.9661\n",
      "Epoch 2/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0778 - accuracy: 0.9745 - val_loss: 0.1060 - val_accuracy: 0.9684\n",
      "Epoch 3/48\n",
      "97/97 [==============================] - 228s 2s/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.1423 - val_accuracy: 0.9571\n",
      "Epoch 4/48\n",
      "97/97 [==============================] - 223s 2s/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.1294 - val_accuracy: 0.9594\n",
      "Epoch 5/48\n",
      "97/97 [==============================] - 224s 2s/step - loss: 0.0304 - accuracy: 0.9890 - val_loss: 0.1137 - val_accuracy: 0.9707\n",
      "Epoch 6/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.1123 - val_accuracy: 0.9639\n",
      "Epoch 7/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0208 - accuracy: 0.9926 - val_loss: 0.1415 - val_accuracy: 0.9616\n",
      "Epoch 8/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0239 - accuracy: 0.9910 - val_loss: 0.1168 - val_accuracy: 0.9661\n",
      "Epoch 9/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.1124 - val_accuracy: 0.9661\n",
      "Epoch 10/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.1255 - val_accuracy: 0.9639\n",
      "Epoch 11/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0180 - accuracy: 0.9942 - val_loss: 0.1255 - val_accuracy: 0.9661\n",
      "Epoch 12/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0121 - accuracy: 0.9945 - val_loss: 0.1443 - val_accuracy: 0.9661\n",
      "Epoch 13/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0107 - accuracy: 0.9958 - val_loss: 0.1279 - val_accuracy: 0.9752\n",
      "Epoch 14/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.1469 - val_accuracy: 0.9684\n",
      "Epoch 15/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.1801 - val_accuracy: 0.9639\n",
      "Epoch 16/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.1352 - val_accuracy: 0.9661\n",
      "Epoch 17/48\n",
      "97/97 [==============================] - 224s 2s/step - loss: 0.0182 - accuracy: 0.9926 - val_loss: 0.1632 - val_accuracy: 0.9594\n",
      "Epoch 18/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.1630 - val_accuracy: 0.9707\n",
      "Epoch 19/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.1465 - val_accuracy: 0.9661\n",
      "Epoch 20/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 0.2003 - val_accuracy: 0.9571\n",
      "Epoch 21/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.1617 - val_accuracy: 0.9639\n",
      "Epoch 22/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.1972 - val_accuracy: 0.9526\n",
      "Epoch 23/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0069 - accuracy: 0.9984 - val_loss: 0.1532 - val_accuracy: 0.9684\n",
      "Epoch 24/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.1703 - val_accuracy: 0.9639\n",
      "Epoch 25/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.1838 - val_accuracy: 0.9616\n",
      "Epoch 26/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0092 - accuracy: 0.9968 - val_loss: 0.1918 - val_accuracy: 0.9594\n",
      "Epoch 27/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.1888 - val_accuracy: 0.9639\n",
      "Epoch 28/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.2237 - val_accuracy: 0.9594\n",
      "Epoch 29/48\n",
      "97/97 [==============================] - 223s 2s/step - loss: 0.0276 - accuracy: 0.9897 - val_loss: 0.2687 - val_accuracy: 0.9571\n",
      "Epoch 30/48\n",
      "97/97 [==============================] - 225s 2s/step - loss: 0.0134 - accuracy: 0.9948 - val_loss: 0.2132 - val_accuracy: 0.9616\n",
      "Epoch 31/48\n",
      "97/97 [==============================] - 227s 2s/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.1744 - val_accuracy: 0.9594\n",
      "Epoch 32/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0159 - accuracy: 0.9958 - val_loss: 0.1938 - val_accuracy: 0.9661\n",
      "Epoch 33/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.2013 - val_accuracy: 0.9616\n",
      "Epoch 34/48\n",
      "97/97 [==============================] - 224s 2s/step - loss: 0.0047 - accuracy: 0.9977 - val_loss: 0.1695 - val_accuracy: 0.9684\n",
      "Epoch 35/48\n",
      "97/97 [==============================] - 225s 2s/step - loss: 0.0177 - accuracy: 0.9948 - val_loss: 0.2413 - val_accuracy: 0.9616\n",
      "Epoch 36/48\n",
      "97/97 [==============================] - 242s 3s/step - loss: 0.0069 - accuracy: 0.9984 - val_loss: 0.2397 - val_accuracy: 0.9684\n",
      "Epoch 37/48\n",
      "97/97 [==============================] - 223s 2s/step - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.2116 - val_accuracy: 0.9616\n",
      "Epoch 38/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.2015 - val_accuracy: 0.9661\n",
      "Epoch 39/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.2056 - val_accuracy: 0.9639\n",
      "Epoch 40/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.2029 - val_accuracy: 0.9616\n",
      "Epoch 41/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.1848 - val_accuracy: 0.9639\n",
      "Epoch 42/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.1887 - val_accuracy: 0.9684\n",
      "Epoch 43/48\n",
      "97/97 [==============================] - 224s 2s/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.2260 - val_accuracy: 0.9707\n",
      "Epoch 44/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0052 - accuracy: 0.9977 - val_loss: 0.1761 - val_accuracy: 0.9661\n",
      "Epoch 45/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0117 - accuracy: 0.9952 - val_loss: 0.1844 - val_accuracy: 0.9684\n",
      "Epoch 46/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 0.1817 - val_accuracy: 0.9729\n",
      "Epoch 47/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0087 - accuracy: 0.9965 - val_loss: 0.1809 - val_accuracy: 0.9752\n",
      "Epoch 48/48\n",
      "97/97 [==============================] - 222s 2s/step - loss: 0.0100 - accuracy: 0.9961 - val_loss: 0.3116 - val_accuracy: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24fc7341ff0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on your data\n",
    "model.fit(trainX, trainY, epochs=number_of_epochs, batch_size = batch_size, validation_data=(valX, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "210164db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a01acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 50s 2s/step - loss: 0.3966 - accuracy: 0.9549\n",
      "Accuray:  0.9548532962799072\n",
      "Loss:  0.39656201004981995\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on your test data\n",
    "loss, accuracy = model.evaluate(testX, testY)\n",
    "print('Accuray: ', accuracy)\n",
    "print('Loss: ', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "dl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
